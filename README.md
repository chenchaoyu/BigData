# BigData
陈潮宇 记录大数据实训每日进度
# 5月21日
为了实现同步，首先要实现检测文件夹变动，然后再进行对应操作。  

查了一些资料发现采用FileAlterationObserver，FileAlterationListener和FileAlterationMonitor可以实现上述目标，这三个类都是org.apache.commons.io.monitor库下的类，采用导入jar的方式可以直接使用。
于是先实现了程序的整体框架，在Main.java里面可以看到。  

紧接着实现了文件的上传和删除功能，即Filesyncer类的uploadFile和deleteFile函数  
# 5月22日
今天主要在实现程序中断后，继续从原来进度上传的功能。首先我把它分成两个部分 

一、程序中断后，原来已经上传的文件不用再次上传，只上传未上传的文件。

二、程序中断后，原来已经上传的分片不用再次上传，只上传未上传的分片。  

两个部分是类似的，但实现起来有一点区别。目前已经实现第一个部分，正在实现第二部分。 采用将未上传的文件路径写入一个log文本文件来实现，同时log文件维持一个状态码。每次启动程序时，检查状态码，若为0，则代表有文件未上传，将列表里的文件依次上传，若为1则表示已上传全部文件，不需要进行任何操作。

对于第二个部分，由于需要从上次中断的分片继续上传，因此需要记录上传的uploadID,已经已完成的分片数。  
# 5月23日
今天完整实现了昨天的第二个部分，即分片中断后可以从上次的进度继续重传。除此之外加入了从键盘获取用户输入的功能，同时加入一些文字提示让程序可用性提高。
另外记录一下一些设计时比较纠结或困扰的点：  

首先就是对于FileSyncer类，由于这个类是用来进行上传或删除文件的，因此经常要调用到，为此采用了单例模式，只维持一个实例，并且开放初始化函数。 

另外就是文件读写，采用的分割符要不常见，最后选了一个"$$$$"符号。感觉最优雅的做法应该是写成类似json数据的格式，但对于java解析这方面的数据没什么经验，抱着能用就好的心态就这么处理了（之后会了解一下这方面）。  

还有一个问题就是感觉可能是一个潜在的bug，如果代码运行到写入log文件时，只写入了一半程序就被终止了，那么这时的log文件是不完整的，下次运行时可能无法正确同步所有文件。解决方法可能是采用类似数据库的事务操作，但是由于时间原因也没有实现。
# 5月25日
完成了程序设计说明书和编译说明的编写，同时组织小组其他成员完成PPT和程序使用说明书。
另外课上学习了大数据的一些架构，例如kafka，Spark，Flink等。记录在课程二文件夹内。
同时准备好了spark开发环境
# 5月26日
课上学习了实操手册中的内容，包括运用spark计算如来在西游记出现了多少次，了解了scala语言；大数据计算和存储分离，登录spark服务器，运行sql语句创建外部表，然后进行查询；大数据打擂台，对比了几个不同架构处理（Hive，Spark，GreenPlum）大数据的能力；将大数据整合到软件，包括使用spark jdbc操作数据库。
完成了扩展题的1，2，3
# 5月27日
完成了扩展题的4
# 5月28日
尝试使用spark jdbc和greenplum jdbc运行一些查询，建表，删表等操作。
# 5月29日
学习了一些scala语言的基础语法
# 6月1日
课上完成了实操5-8，课下完成了实操5-7的扩展题，扩展题8还有点问题。
# 6月2日
课上完成了分册3实操1-3。课下完成了实操1,3扩展题。为了完成扩展题二，查看了flink的api文档，发现需要将数据按窗口进行处理，然后运用datastream的聚合函数进行统计。但是目前不知道如何排序，正在查找资料解决问题。另外组织小组开始进行大作业开发，先完成了后端的demo开发，用postman进行接口测试。已完成的部分扩展题放在课程2和3中。
# 6月3日
今天完成了扩展题8。另外主要的时间在进行大作业程序的开发。我负责的是后端接口部分，完成了数据库查询的部分接口，开发过程中的主要问题
1)spark jdbc的依赖一直出错，由于我们没有直接使用老师给的spark jdbc包，而是用maven，在依赖上花费了不少时间。
2)需要和前端成员沟通好接口的需求。
3)由于尝试了之前没有用过的框架springboot,花了一点时间入门。
另外和负责前端的组员进行了沟通，发现还需要补充一些接口，明天会继续开发。
# 6月4日
今天完成了一部分新的接口，同时整合了其他两位同学做的接口。
# 6月5日
今天完成了扩展题2。大作业方面由于前端开发还没有完成，剩下的时间主要用于完成其他课程的作业。
# 6月8日
课上跟着老师完成了实操4-7的内容。对实时计算有了更深刻的了解。课下完成实操4的扩展题，将Flink流计算的数据流入mysql。同时开始思考扩展题5和大作业的内容。大作业已经有了一点思路，首先写一个producer类，用来将数据从s3导入kafka；然后再写一个consumer类，用来处理数据，按关键值分组；还需要一个writer类用来将处理好的数据写入文件，并上传到s3，这部分可以参考实操四的内容，也是将数据上传到s3桶。
# 6月9日
课上首先听老师讲了一点人工智能的历史，现在和未来，对人工智能有了一个更广阔的认识。之后完成了关于人工智能的几个简单的程序，对于数据清理的重要性有了更深刻的理解。课下完成了实操五的扩展题和大作业。大作业部分正在写说明书，之后会将源码和截图一并上传到这个仓库。
# 6月10日
今天主要在看机器学习的内容，算是和课上内容有关吧
# 6月11日
主要完成昨天课上的扩展题，包括将kafka数据导入mysql，采用了连接本地数据库的做法。
# 6月12日
今天主要在看NLP的内容，同时详细看了一下课上那几个py的代码。
# 6月15日
课上完成了人工智能实操4-7，完成了扩展题。对机器学习有了更深刻的了解。
# 6月16日
课上对数据治理有了初步的认知，包括领域模型的建立，数据各字段直接，各表之间的关系的建模等。同时开始着手写人工智能课程的大作业的代码，使用的是house price这份数据，即根据房屋信息对房屋的价格做一个回归预测。


